# -*- coding: utf-8 -*-
"""Interfuser

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OP1QGDtLSaTUQl7-6jZwyXFrA1BsNj6T

#Speed Head
"""

import torch
import torch.nn as nn

class SpeedHead(nn.Module):
    def __init__(self):
        super(SpeedHead, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),  # Additional conv layer
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.flatten = nn.Flatten(start_dim=1)  # Flatten along channels dimension

        # Calculate the input size for the linear layers after flattening
        self.linear_input_size = 10240

        self.linear_layers = nn.Sequential(
            nn.Linear(self.linear_input_size, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # Output layer with 1 neuron
        )

    def forward(self, x):
        x = self.conv_layers(x)
        flattened_x = self.flatten(x)
        print("Flattened shape:", flattened_x.shape)  # Print the shape of the flattened tensor
        x = self.linear_layers(flattened_x)
        return x

# Create an instance of the SpeedHead model
model = SpeedHead()

# Create a random input tensor with shape [batch_size, channels, height, width]
input_tensor = torch.randn(32, 1, 165, 256)

# Forward pass through the model
output_tensor = model(input_tensor)

# Print the shape of the output tensor
print("Output shape:", output_tensor.shape)

"""#Break Head"""

import torch
import torch.nn as nn

class BreakHead(nn.Module):
    def __init__(self):
        super(BreakHead, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)  # Add one more conv layer
        )

        self.flatten = nn.Flatten(start_dim=1)  # Flatten along channels dimension

        # Calculate the input size for the linear layers after flattening
        self.linear_input_size = 10240

        self.linear_layers = nn.Sequential(
            nn.Linear(self.linear_input_size, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # Output layer with 1 neuron
        )

    def forward(self, x):
        x = self.conv_layers(x)
        flattened_x = self.flatten(x)
        print("Flattened shape:", flattened_x.shape)  # Print the shape of the flattened tensor
        x = self.linear_layers(flattened_x)
        return x

# Create an instance of the BreakHead model
model = BreakHead()

# Create a random input tensor with shape [batch_size, channels, height, width]
input_tensor = torch.randn(32, 1, 81, 256)

# Forward pass through the model
output_tensor = model(input_tensor)

# Print the shape of the output tensor
print("Output shape:", output_tensor.shape)

"""#WayPoint"""

import torch
import torch.nn as nn

class WaypointCNN(nn.Module):
    def __init__(self):
        super(WaypointCNN, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # Adjust the pooling operation
            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # Adjust the pooling operation
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # Adjust the pooling operation
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),  # Adjust the pooling operation
            nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, stride=1, padding=1),  # Add one more conv layer
            nn.ReLU(),
        )

    def forward(self, x):
        x = self.conv_layers(x)
        return x.squeeze(1)  # Squeeze the output tensor along the channel dimension

# Create an instance of the WaypointCNN model
model = WaypointCNN()

# Create a random input tensor with shape [batch_size, channels, height, width]
input_tensor = torch.randn(32, 1, 165, 256)

# Forward pass through the model
output_tensor = model(input_tensor)

# Print the shape of the output tensor
print("Output shape:", output_tensor.shape)

import torch

# Create a tensor with shape [32, 1, 10, 256]
original_tensor = torch.randn(32, 1, 10, 256)

# Reshape the tensor to [32, 10, 256]
reshaped_tensor = original_tensor.view(32, 10, 256)
# Alternatively, you can use reshape method
# reshaped_tensor = original_tensor.reshape(32, 10, 256)

# Print the shapes of the original and reshaped tensors
print("Original shape:", original_tensor.shape)
print("Reshaped shape:", reshaped_tensor.shape)

import torch

# Create a reshaped tensor with shape [32, 10, 256]
reshaped_tensor = torch.randn(32, 10, 256)

# Revert the reshaped tensor back to the original shape using view
original_tensor = reshaped_tensor.view(32, 1, 10, 256)

# Print the shapes of the reshaped and original tensors
print("Reshaped shape:", reshaped_tensor.shape)
print("Original shape:", original_tensor.shape)